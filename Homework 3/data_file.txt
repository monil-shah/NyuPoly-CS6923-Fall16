{"abstract": "Registration and Breakfast", "title": "Registration and Breakfast", "description": "Registration and Breakfast", "time": "Friday 8 a.m.-9 a.m."}
{"abstract": "See description. ", "title": "Keynote: Computer Science: America's Untapped Opportunity ", "description": "Software and computers are everywhere, revolutionizing every field around us. But the majority of schools don't teach computer science. Code.org believes every student should have the opportunity to shape the 21st-century and wants to turn this problem around. Code.org has already helped over 100 million students try computer science for the first time with one Hour of Code, partnered with 70+ school districts to bring courses to schools, released a free online learning platform with 10% of students in K-8 US schools enrolled. We've helped change policy in 16 states to better support computer science.  This is just the beginning of a bold vision to bring this foundational field to every K-12 public school by 2020.", "time": "Friday 9 a.m.-9:50 a.m."}
{"abstract": "Machine learning is the branch of computer science concerned with the development of algorithms which can be trained by previously-seen data in order to make predictions about future data. It has become an important aspect of work in a variety of applications: from optimization of web searches, to financial forecasts, to studies of the nature of the Universe. This tutorial will explore machine learning with a hands-on introduction to the scikit-learn package. Beginning from the broad categories of supervised and unsupervised learning problems, we will dive into the fundamental areas of classification, regression, clustering, and dimensionality reduction. In each section, we will introduce aspects of the Scikit-learn API and explore practical examples of some of the most popular and useful methods from the machine learning literature. The strengths of scikit-learn lie in its uniform and well-document interface, and its efficient implementations of a large number of the most important machine learning algorithms. Those present at this tutorial will gain a basic practical background in machine learning and the use of scikit-learn, and will be well poised to begin applying these tools in many areas, whether for work, for research, for Kaggle-style competitions, or for their own pet projects.", "title": "Machine Learning with Scikit-Learn", "description": "This tutorial will offer an introduction to the core concepts of machine learning and the Scikit-Learn package. We will introduce the scikit-learn API, and use it to explore the basic categories of machine learning problems and related topics such as feature selection and model validation, and practice applying these tools to real-world data sets.", "time": "Friday 10 a.m.-noon"}
{"abstract": "The first part of the tutorial will cover basic data science concepts and use code and data examples relevant to data science (drawn from the UCI mushroom dataset). Basic Python programming concepts will include data structures (strings, lists, tuples, dictionaries), control structures (conditionals & loops), file I/O, and defining and calling functions. The second part of the tutorial will focus on constructing a simple decision tree based on the ID3 algorithm and using it to classify instances from the UCI mushroom dataset. This portion will also include the use of recursion, Python classes (object-oriented programming) and the use of Python scripts with arguments from the command line.", "title": "Python for Data Science: A Rapid On-ramp Primer", "description": "The goal of this tutorial is to provide efficient and sufficient scaffolding for people with no prior knowledge of Python - but with some knowledge of programming - to effectively utilize Python-based tools for data science research and development, such as the pandas and scikit-learn open source libraries, or the Atigeo xPatterns analytics framework.", "time": "Friday 10 a.m.-noon"}
{"abstract": "When I started learning more about statistics I became very frustrated with the numerous specialized tests and statistical measures. They all have their place but they tend to make an already intimidating field even more intimidating. As a result, I began to figure out methods of validating what I was doing that were based more on repeatable simulation rather than theory. I was then able to show working code to fellow analysts that illustrated why I was doing the statistical analyses I was doing and they could replicate and tweak my simulations to explore the possibilities rather than debate. At it's heart, this is a tutorial about how to have more constructive conversations about statistical inference with your peers. I will walk attendees through the topic via an Python notebook and all charting will be done in front of them as well using either Seaborn or Matplotlib. The core of the topic revolves around using distributions of data in order to drive all of our inference. I will take a few minutes in the beginning to prove out (via simulation) that these simulations on the data distributions are perfectly equivalent to their probability distributions. In other words, a beta distribution can be modeled using a distribution of 0's and 1's, a normal distribution can be developed sampling means randomly from observed static data, etc. I will cover the following topics:  Simulations and Monte Carlo methods Parametric vs. non-parametric statistical tests Bootstrapping Solve probability puzzles with simulation Answer 'How many samples does this experiment need?' Find split test conversion lift using simulation From here to bayesian statistics (quick shout out to PyMC) ", "title": "Simplified statistics through simulation", "description": "We will learn how to make valid statistical inferences using only Python/Numpy in a way that is easy to understand. You shouldn't need to put blind faith in your local statistical expert. This tutorial will show you how to use less theory and validate your methods concretely using simulation.", "time": "Friday 10 a.m.-noon"}
{"abstract": "The tutorial will cover: PySnpTools details:   PstReader: Full NumPy-meets-Pandas-like slicing and subsetting of matrix data before (and after) reading from disk. (For genomics, it includes support for the PLINK Bed and phenotype formats. It also includes low-memory, high-speed methods for common operations such as standardization and kernel-creation.)   Utilities: One line intersecting and re-ordering of data for machine learning and statistics. Faster-than-NumPy extraction of a subarray from a NumPy array.    IntRangeSet: Manipulate from zero to billions of integers as sets with very little memory.   Python Trade Offs We Observe: Our industrial research group focuses on Machine Learning. Over 15 years, we have moved from C++/VB to C# to Python. I'll talk about why we choose Python and what tradeoffs we see. Application: PySnpTools spun out of FaST-LMM. FaST-LMM is an Open Source, Python-based state-of-the-art system for doing Genome Wide Association Studies (GWAS). It is described in publications in Nature Methods, Nature Genetics, and Bioinfomatics. I'll talk about:  a layman's overview of GWAS how to use FaST-LMM how FaST-LMM uses PySnpTools  To Install:  pip install pysnptools Documentation: http://research.microsoft.com/en-us/um/redmond/projects/MSCompBio/PySnpTools/ Source: https://github.com/MicrosoftGenomics/PySnpTools ", "title": "PySnpTools: A New Open-Source Library for Reading and Manipulating Matrix Data (including Genomics)", "description": "Anyone who uses fast numeric NumPy arrays but would like a simpler-than-Pandas ability to slice-and-dice, read-and-write will find PySnpTools useful. I'll describe PySnpTools and also tell how it fits into our Machine Learning research group's long-term move from C++/VB to C# to Python. I'll also show how we use PySnpTools in FaST-LMM to do state-of-the-art Genome Wide Association Studies.", "time": "Friday 10 a.m.-noon"}
{"abstract": "Lunch", "title": "Lunch", "description": "Lunch", "time": "Friday noon-1 p.m."}
{"abstract": "As companies scale prototypes and ad hoc analyses into production systems, it is critical to build automated (and repeatable) systems for data collection/processing and model training /evaluation which are fault tolerant enough to adapt to changing constraints.  Sustainable software development is often an afterthought for data scientists, especially since the tools for analysis (R, scientific python, etc.) do not naturally lend themselves to building scalable and extensible software abstractions.  But now we can have our cake and eat it too... all with Python! In this workshop you see how (and why) to leverage the PyData ecosystem to build a robust data pipeline.  More specifically you will learn how to use the Luigi framework to integrate multiple stages of a model building pipeline: collection, processing, vectorization, training of multiple models, and validation. Outline:  The basic components of a data pipeline (5min) What and Why Luigi (10min) Lab: The Smallest (1 stage) pipeline (15min) Managing dependencies in a pipeline (10min) Lab: Multi-stage pipeline and introduction to the Luigi Visualizer (15min) Serialization in a Data Pipeline (10min) Lab: Integrating your pipeline with HDFS and Postgres (20min) Scheduling (10min) Lab: Parallelism and recurring jobs with Luigi (20min) Wrap up and next steps (5min) ", "title": "Scalable Pipelines w/ Luigi or: I'll have the Data Engineering, hold the Java!", "description": "In this workshop you see how (and why) to leverage the PyData ecosystem to build a robust data pipeline.  More specifically you will learn how to use the Luigi framework to integrate multiple stages of a model building pipeline (collection, processing, vectorization, training of multiple models, and validation) all in Python!", "time": "Friday 1 p.m.-3 p.m."}
{"abstract": "The motivation of this tutorial mirrors that of pandas itself: practicality. A brief discussion on the problems pandas tries to solve will help frame the rest of the tutorial. We'll aim for an intuitive understanding of each new method and data structure. This will help keep us from getting overwhelmed by the options available as we expand our data munging toolkit. The start of the talk will focus on the core operations of  Selecting and Indexing Reshaping and Tidy Data Summarization Grouped operations Merging and Joining  These operations can be combined into 'pandastic' method chains that flow seamlessly from data IO to analysis. Time permitting we'll look at some of the more specialized areas of pandas including Categoricals, time-series analysis, Hierarchical Indexes, chunked / out of core processing, and data pipelines. Learning to use a library the size of pandas is a huge commitment. What's more, your goal is rarely achieved just with pandas. Rather, pandas gets you to the point where you can begin your interesting analysis. We'll build the foundation to quickly get you past the data munging, to the analysis.", "title": "Pandas: .head() to .tail()", "description": "Pandas is an extremely powerful library for data analysis. With that power comes complexity. This tutorial will focus on the core features of pandas, which handle most data munging tasks. The empahsis will be on practical applications, illustrating solutions to common problems using real-world data.", "time": "Friday 1 p.m.-3 p.m."}
{"abstract": "Coming soon.", "title": "Beautiful Interactive Visualizations in the Browser with Bokeh", "description": "Coming soon.", "time": "Friday 1 p.m.-3 p.m."}
{"abstract": "Apache Spark is a fast and general engine for distributed computing & big data processing with APIs in Scala, Java, Python, and R. This tutorial will briefly introduce PySpark (the Python API for Spark) with some hands-on-exercises combined with a quick introduction to Spark's core concepts. We will cover the obligatory wordcount example which comes in with every big-data tutorial, as well as discuss Spark's unique methods for handling node failure and other relevant internals. Then we will briefly look at how to access some of Spark's libraries (like Spark SQL & Spark ML) from Python. While Spark is available in a variety of languages this workshop will be focused on using Spark and Python together. This tutorial is intended for people new to Spark/PySpark, please install Spark (1.3.1 or later) from http://spark.apache.org/downloads.html before class (we are working to have cluster resources available but having a local install is sufficient for the workshop and a good backup in case the WiFi isn't cooperating).", "title": "A brief introduction to Distributed Computing with PySpark", "description": "N/A", "time": "Friday 1 p.m.-3 p.m."}
{"abstract": "Break and Snacks ", "title": "Break and Snacks ", "description": "Break and Snacks", "time": "Friday 3 p.m.-3:20 p.m."}
{"abstract": "NumPy and Pandas provide excellent in-memory containers and computation for the Scientific Python ecosystem.  As we extend to larger-than-memory datasets these containers fail, leaving scientists with less productive options that mesh less well with the existing ecosystem. A common solution to this problem is blocking algorithms and task scheduling. Blocking algorithms define macro-scale operations on the full dataset as a network of smaller operations on in-memory blocks of the dataset.  Task scheduling allows many parallel workers to execute these tasks in a way consistent to their data dependencies. We introduce dask, a task scheduling specification, and dask.array a high-level abstraction that implements a large subset of the NumPy API with blocked algorithms.  In many cases dask.array provides a drop-in replacement for NumPy for out-of-core datasets with parallel execution.  We discuss the design choices behind dask, dask.array, and related projects and show performance both quantitatively with benchmarks and also in usability by demonstrating integration into the larger ecosystem.", "title": "Dask: out-of-core arrays with task scheduling", "description": "Dask Array implements the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. We describe dask, dask.array, dask.dataframe, as well as task scheduling generally.", "time": "Friday 3:20 p.m.-5:20 p.m."}
{"abstract": "As the amount of Unstructured Linguistic Data is increasing each day, it is becoming important to develop tools to analyze this data automatically. In this tutorial I will take you through the basics of linguistic data analytics and then build up to come more complicated pieces of NLP. We will start with basic linguistic techniques - such as Lemmatization, Part of Speech Tagging, Parsing etc, and write some code to implement some these using NLTK. Next, I will talk about how probabilities and statistics are used with Linguistic Data Processing to develop Language Models, and finally we will talk about more complicated techniques such as Deep Learning. If we have time, we will go over two use of NLP - search engines, and sentiment analysis of customer reviews", "title": "Using Python for Linguistic Data Analysis", "description": "This tutorial is an introduction to Natural Language Processing using Python, to rapidly build your own NLP module. We  will start with the very basics of NLP - Lemmatization, Stemming, POS tagging, Parsing, Language Models, to the more complex  pieces of NLP involving probabilities, statistics and word co-occurrences and finally deep learning approaches to NLP and  word vectorization techniques. ", "time": "Friday 3:20 p.m.-5:20 p.m."}
{"abstract": "We start out with a description of what deep learning is and give examples of applications where it's been successful. Then we walk through the setup of the GraphLab Create package. In the second half of the tutorial, we walk through a IPython notebook that constructs an image similarity search application using a deep learning model and a nearest neighbors toolkit. If time allows, we will discuss how to combine image features and text features to improve the quality of search results. Folks should walk away knowing how to start building an image analysis application on their own using deep learning.", "title": "Learn to Build an App to Find Similar Images using Deep Learning", "description": "Deep Learning is a powerful machine learning method for image tagging, object recognition, speech recognition, and text analysis. In this hands-on tutorial, we'll teach you the basic concept of deep learning and walk you through the steps to build an application that finds similar images using a deep learning model.", "time": "Friday 3:20 p.m.-5:20 p.m."}
{"abstract": "This tutorial offers an introduction to how we perform change detection on data streams at Netflix using Python. We will introduce the problem and a framework for solving it, as well as a method for evaluating the effectiveness of different techniques. Once we've established the problem and a framework we will dive into some real-world data collected at Netflix about device call volume in our call centre, and attempt to detect a meaningful increase in call volume related to a device. The associated notebook will scaffold out the framework, then introduce simple techniques, slowly building in complexity. Time permitting we will discuss these techniques and potentially the usefulness of an ensemble of techniques in a production environment to provide robustness against different behavior patterns of data. Topics Covered: - Change deteciton problem definition and framework - Simple change detection techniques - Synthetic data generation - Real-world data evaluation", "title": "Real-Time Change Detection on Streaming Data (Sponsor Tutorial)", "description": "This tutorial offers an introduction to how we perform change detection on data streams at Netflix using Python. We will develop a framework for running and evaluating change detection algorithms and then experiment with various techniques on real-world data gathered at Netflix.", "time": "Friday 3:20 p.m.-5:20 p.m."}
{"abstract": "Registration and Breakfast ", "title": "Registration and Breakfast", "description": "Registration and Breakfast", "time": "Saturday 8 a.m.-9 a.m."}
{"abstract": "Several exciting trends are driving the birth of the intelligent cloud. The vast majority of world's data is now connected data resident in the cloud. The majority of world's new software is now connected software, also resident in or using the cloud. New cloud based Machine Learning as a Service platforms help transform data into intelligence and build cloud-hosted intelligent APIs for connected software applications. Face analysis, computer vision, text analysis, speech recognition, and more traditional analytics such as churn prediction, recommendations, anomaly detection, forecasting, and clustering are all available now as cloud APIs, and far more are being created at a rapid pace. Cloud hosted marketplaces for crowdsourcing intelligent APIs have been launched. In this talk I will review what these trends mean for the future of data science and show examples of revolutionary applications that you can build using cloud platforms.", "title": "Keynote: Clouded Intelligence", "description": "Keynote", "time": "Saturday 9 a.m.-9:45 a.m."}
{"abstract": "Pandas is a fast and expressive library for data analysis that doesn't naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. This talk introduces Sparkling Pandas, a library that brings together the best features of Pandas and PySpark; Expressiveness, speed, and scalability. While both Spark 1.3 and Pandas have classes named 'DataFrame' the Pandas DataFrame API is broader and not fully covered by the 'DataFrame' class in Spark. This talk will explore some of the differences between Spark's DataFrames and Panda's DataFrames and then examine some of the work done to implement Panda's like DataFrames on top of Spark. In some cases, providing Pandas like functionality is computationally expensive in a distributed environment, and we will explore some techniques to minimize this cost. At the end of this talk you should have a better understanding of both Sparkling Pandas and Spark's own DataFrames. Whether you end up using Sparkling Pandas or Spark directly, you will have a greater understanding of how to work with structured data in a distributed context using Apache Spark and familiar DataFrame APIs.", "title": "Sparkling Pandas - Letting Pandas Roam on Spark DataFrames", "description": "Pandas is a fast and expressive library for data analysis that doesn't naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. This talk introduces Sparkling Pandas, a library that brings together the best features of Pandas and PySpark.", "time": "Saturday 9:50 a.m.-10:30 a.m."}
{"abstract": "The Open-Source Policy Center (OSPC) seeks to make policy analysis more transparent, trustworthy, and collaborative by harnessing open-source methods to build cutting-edge economic models. Our first package for release is the Tax Calculator. This Python package encodes current federal tax law and can be used to assess how policy reforms will affect government revenue and the distribution of the tax burden across income groups. In order to make this resource available to a large audience, we have created TaxBrain, a web application that allows users to specify Tax Calculator computations through a browser. The results are displayed in the browser as a number of tables, downloadable as CSV files. In this talk, we discuss the architecture of this web app, and its deployment on the Heroku platform. Tax Brain is a unique combination of web-enabled and traditional 'scientific stack' Python code. We discuss our lessons learned, and give advice for those who wish to deploy numerical calculation codes in web-accessible environments.", "title": "Building TaxBrain: Numba-enabled Financial Computing on the Web", "description": "The Open Source Policy Center maintains a Python package ('Tax Calculator') that uses Numba to model the federal income tax code for policy analysis. In this talk, we describe the construction of TaxBrain, a web app deployed on Heroku that allows non-programmers to use this package. We discuss the particulars of handling computationally intensive workloads with compiled code on a cloud platform.", "time": "Saturday 9:50 a.m.-10:30 a.m."}
{"abstract": "This talk focuses on the methodology and intent of studying feedback form data using Python tools and libraries for natural language processing and machine learning analysis. I will discuss potential trouble areas of starting such a project from scratch from a developer's perspective. This will include the type of analysis that might be helpful in discerning user experience and what analysis that you run, but might end up tossing out at the end due to lack of insight on your data. For instance, what is the value of running a K-Means cluster analysis and does it offer substantial actionable insights for textual content? And how much data do you need to pre-label for a training set for a Naive Bayes Classification in order for it to be accurate? My intent is that people will walk away learning the basics of textual analysis and become motivated to help their users succeed in whatever tasks they are trying to accomplish through finding potential points of friction and even issues that spring up from changes in the design. This talk will be for developers or marketers who don't have a lot or any experience in data analysis or machine learning.", "title": "Investigating User Experience with Natural Language Analysis", "description": "This talk focuses on the methodology and intent of studying feedback form data using Python tools and libraries for natural language processing and machine learning analysis. I will discuss potential trouble areas of starting such a project from scratch from a developer's perspective and will include the type of analysis that might be helpful in discerning user experience.", "time": "Saturday 9:50 a.m.-10:30 a.m."}
{"abstract": "This talk presents an overview of some of the tools, technologies and programs being developed and offered by Intel(r) for the Data community and Python Developers. The Intel(r) Data Analytics Acceleration Library (Intel(r) DAAL) is a C++ and Java software solution for data analytics. The library provides a set of optimized building blocks that can be used in all stages of the data analytics workflow. A prototype Python profiler will be featured, with details on the highlights and advantages of this tool. Another project in the works is a Python Distribution for scientific computing and data analysis, that leverages the power of Intel(r) Math Kernel library (Intel(r) MKL) to deliver faster performance. A quick spotlight on the latest features and additions to the Intel (r) Performance Libraries - Intel(r) MKL and Intel(r) Integrated Performance Primitives (Intel(r) IPP). And finally, a fresh look at the Intel free tools software program, to discover some of the free offerings for academic researchers, educators, students and open source contributors.", "title": "Accelerate data analytics and Python performance with Intel(r) software (Sponsor Talk)", "description": "This talk presents an overview of some of the tools, technologies and programs being developed and offered by Intel(r) for the Data community and Python Developers. We'll highlight the new Intel(r) Data Analytics Acceleration Library (Intel(r) DAAL), a prototype Python profiler, a Python Distribution for scientific computing & data analysis, Intel(r) MKL, Intel(r) IPP and the free software tools program. ", "time": "Saturday 9:50 a.m.-10:30 a.m."}
{"abstract": "Coming Soon", "title": "Memex: Mining the Dark Web", "description": "Coming Soon", "time": "Saturday 10:40 a.m.-11:20 a.m."}
{"abstract": "Experimentation (A/B testing) is a hot topic in online services. While there is ample discussion around why you should test and the benefits it might bring there is a surprising lack of discussion around actually implementing or building an experimentation platform. From an engineering perspective, how do I go from a blog post on A/B testing to a full fledged platform that provides my company robust, trustworthy, and scalable experimentation? In this talk, I will demonstrate the power of simulating user interaction logs (written in python) as building blocks for a test driven approach to constructing various parts of an experimental platform.  I will mainly focus on the aggregation of such logs into interpretable scorecards fit for non-technical consumption.  I will demonstrate the accuracy and flexibility of simulated logs in both reproducing real world outcomes as well as providing methods for testing unseen scenarios.  I will also touch on user randomization, the pitfalls of incorrect aggregations, and provide an abstract way to think about experimentation in general. Finally, I will comment on a few of the (very public) ways that exeperimentation has been shown to fail and, with the use of simulated users, provide possible explanations for the failures. The goal of this talk is to provide evidence of the usefulness of log simulation, demonstrate the very simple concepts behind building the non-operational components of an experimentation system, and hopefully impart some experimental intuitions to attendees not deeply familiar with online experimentation.", "title": "Who needs users? Just simulate them!", "description": "How do I build a robust, trustworthy, and scalable experimentation system when my product doesn't have the millions of users I expect it to have in the future?  Simulation of course! In this talk, we will explore components of an experimentation platform through modular simulation units that provide a shockingly realistic picture user logs and wade through the pitfalls of A/B testing.", "time": "Saturday 10:40 a.m.-11:20 a.m."}
{"abstract": "Standard natural language processing (NLP) is a messy and difficult affair. It requires teaching a computer about English-specific word ambiguities as well as the hierarchical sparse nature of words in sentences. At Stitch Fix, word vectors help computers learn from the raw text in customer notes. Our systems need to identify a medical professional when she writes that she 'used to wear scrubs to work' and distill 'taking a trip' into a Fix for vacation clothing. Applied appropriately, word vectors are dramatically more meaningful and more flexible than current techniques and let computers peer into text in a fundamentally new way. I'll speak about word2vec and related techniques and will try to convince you that word vectors give us a simple and flexible platform for understanding text.", "title": "A Word is Worth a Thousand Vectors", "description": "I'll speak about word2vec, LDA and related text techniques available in Python.", "time": "Saturday 10:40 a.m.-11:20 a.m."}
{"abstract": "Problems with existing Python and C++ debuggers when mixing two code types: no ability to step between the two, no way to peek at underlying native data structures under the hood when in Python. Seamless mixed-mode debugging of both types in a single debug session is necessary to solve these problems, including transparent stepping from Python to native code and from native to Python, breakpoints in both types of code side by side, and the ability to inspect how the objects look from the other side. The talk showcases the overall mixed-mode experience in Python Tools for Visual Studio, and then goes over all the specific features listed above in detail, and the limitations imposed by the mixed-language model.", "title": "Mixed-language Python/C++ debugging with Python Tools for Visual Studio (Sponsor Talk)", "description": "A well-recognized strength of Python is the ease with which it can be extended with libraries written in C. At the same time, the debugging story for such extensions has been notoriously poor, with no ability to seamlessly debug both languages at the same time. This talk showcases an implementation of such seamless debugging in Python Tools for Visual Studio.", "time": "Saturday 10:40 a.m.-11:20 a.m."}
{"abstract": "The Institute for Health Metrics and Evaluation has been pushing the science of global health forward by introducing cutting edge statistical and computational techniques to a rapidly growing collection of health data from around the world. I will demonstrate this through several examples of how Python fits into our large data analysis stack:  PyMB is a model building tool that uses algorithmic differentiation to allow us to optimize large statistical models. Previous Bayesian modeling frameworks were unable to fit such large models quickly (or sometimes not at all), so this has enabled us to greatly enhance the quality of our models.  V1 uses IPython magic on top of rpy2 to abstract away the complexities of writing TMB models.  V2 is in progress and uses PyCppAD to create a Pythonic interface for generating highly efficient C++ models that uses numpy for data I/O.   DisMod is a disease modeling package that uses PyMC to fit compartmental models to 'messy' data. CODEm is an ensemble modeling framework that can test thousands of hypothetical models and generate optimal combinations using crossvalidation. V1 used Python to glue together a lot of Stata code that had previously been exceptionally difficult to run on our 17k core cluster. V2 is entirely rewritten in Python and uses multithreading and Theano to speed up the previous Stata implementation by over 100x.   We have begun a new project to forecast the entire Global Burden of Disease to 2040 and enable policymakers and funders to decide how to best ensure the health of the world in the future. This project uses PySpark to manage simulations outputting over 3 petabytes of data each time they're run. We have built a tool to enable us to create directed acyclic graphs from SymPy expressions that can be executed seamlessly on backends ranging from single threaded numpy packages to large Hadoop clusters.   Finally, I'll touch on how we use web tools such as GBD Compare in our work to both share results and enable collaborators around the world to easily run sophisticated models on our cluster using simple GUIs. These tools are primarily javascript on the front end, but most leverage Django on the backend to allow our developers to quickly prototype and integrate with our statistical software. ", "title": "Saving Lives with Data: Python and Global Health", "description": "At the University of Washington's Institute for Health Metrics and Evaluation we combine a massive collection of global health data with cutting edge statistics to inform decision making that potentially affects the health of billions of people. I'll explain how Python is an integral part of our stack for managing petabytes of data and introduce several novel statistical tools we've developed.", "time": "Saturday 11:30 a.m.-12:10 p.m."}
{"abstract": "As an engineer, analyst, or scientist, sharing your work with someone outside of your immediate team can be a challenge. End-users embody many roles with a wide range of technical skill and often times no familiarity with Python or the command line. Findings, key results, and models are frequently boiled down to static graphs, tables, and figures presented in short reports or slideshow presentations. However, engaging research and data analysis is interactive, anticipating the users' questions and giving them the tools to answer those questions with a simple and intuitive user interface. Browser based applications are an ideal vehicle for delivering these types of interactive tools, but building a web app requires setting up backend applications to serve up content and creating a UI with languages like HTML, CSS, and JavaScript.  This is a non-trivial task even for web-developers and can be completely overwhelming for anyone not familiar with web stack basics. Spyre is a web application framework for the python developer who may have little knowledge of how web applications works, much less how to build them. Spyre takes care of setting up both the front and back-end of your web application. It uses CherryPy to handle HTTP request logic and Jinja2 to auto-generate all of the client-side nuts and bolts, allowing developers to quickly move the inputs and outputs of their python modules into a browser based application. Inputs, controls, outputs, and the relationships between all of these components are specified in a python dictionary. The developer need only define this dictionary and override the methods needed to generate content (text, tables, and plots). While Spyre apps are launched on CherryPy's production-ready server, Spyre's primary goal is to provide a development path for simple light-weight apps without the need for a designer or front-end engineer. For example, Spyre can be used for  rapid prototyping and building MVPs  data exploration developing educational resources building monitoring tools presenting interactive scientific or analytical results to a non-technical audience  just to name a few.", "title": "From DataFrames to Interactive Web Applications in 10 minutes", "description": "Any data driven projects can benefit greatly from a simple, interactive, and easily accessible user interface. Whether your project is in the prototyping stage or you just want a way to quickly get your ideas and research to an audience unfamiliar with the command line, this talk will show you how to quickly turn your python code into interactive web applications. ", "time": "Saturday 11:30 a.m.-12:10 p.m."}
{"abstract": "If you're collecting time-series data (e.g. heart rate, stock prices, server usage, temperature) the fourier transform can be a useful tool for analyzing the underlying periodic nature of the data. But, what is it actually doing? In this talk we'll start from the foundation of basic geometry and explain what the transform is doing. The talk will feature lots of animated graphics to take the mystery out of this powerful method ... and to keep you from reading Twitter during the talk. We'll look at example applications and example code on how to use it in practice, along with practical tips, like choosing the number of bins and what in the world 'windowing' functions are.", "title": "An Intuitive Introduction to the Fourier Transform and FFT", "description": "The 'fast fourier transform' (FFT) algorithm is a powerful tool for looking at time-based measurements in an interesting way, but do you understand what it does? This talk will start from basic geometry and explain what the fourier transform is, how to understand it, why it's useful and show examples.", "time": "Saturday 11:30 a.m.-12:10 p.m."}
{"abstract": "Coming Soon", "title": "Anaconda Cluster Use Case (Sponsor Talk)", "description": "Coming Soon", "time": "Saturday 11:30 a.m.-12:10 p.m."}
{"abstract": "Lunch", "title": "Lunch", "description": "Lunch", "time": "Saturday 12:10 p.m.-1:05 p.m."}
{"abstract": "Coming Soon", "title": "Keynote", "description": "Coming Soon", "time": "Saturday 1:05 p.m.-1:50 p.m."}
{"abstract": "This is a talk for anyone who wants a more diverse engineering culture at work. If you've ever been frustrated by the sameness of your engineering peers, you'll hear practical advice you can use immediately. Diverse engineering teams recruit the best talent, are more innovative, better reflect the needs of their users and make for incredibly fun places to work. Anyone can advocate for their engineering department to be more inclusive of diverse genders, races and backgrounds. It's not a 'woman problem', a 'gay problem' or a 'minority problem' - it's a community problem.  Measure - You need data to measure progress, and the first step is knowing how diverse your group really is. Fund - Getting the funds and providing logistical support to host meetups for diverse groups in your city Raise - Raise your hand and let your company's management know that the issue is important to you and to your colleagues. Call Out - When you hear something misogynistic, homophobic or racist, say so. Don't let the responsibility fall solely on the group of people targeted. Recruit - Actively recruit for diversity in your department. ", "title": "Straight, White Males Should Advocate for Diversity", "description": "This is a talk for anyone who wants a more diverse engineering culture at work. If you've ever been frustrated by the sameness of your engineering peers, you'll hear practical advice you can use immediately. Diverse engineering teams recruit the best talent, are more innovative, better reflect the needs of their users and make for incredibly fun places to work.", "time": "Saturday 1:50 p.m.-2:30 p.m."}
{"abstract": "Scientific Background Supernovae Type Ia which are exploding stars of a certain class have been considered to be emperical standardizable candles: astrophysical objects whose intrinsic luminosity can be inferred from other observable characteristics of their light after explosion. This property was used in the discovery that the expansion of the universe is accelerating.  Extracting this information from the data requires a number of analysis steps;  it is important to try out different methods in many of these steps to discover the optimal methods, thereby requiring a certain amount of modularity in the cod e. The information extracted also depends on the observational strategy used in the survey, and thus exploring the best observational strategy is also important. Implementation The readily available and growing library of python scientific software for data analysis provides a good environment for quickly and easily implementing new algorithms, along with the ability to call programs in other languages. Our suite uses a python package SNCosmo to provide basic functionality for several tasks related to supernovae analysis. We also use products from the suite of LSST software to characterize the observational strategy. Finally, we use a workflow management tool Tigres to build larger tasks out of the primitives, and parallelize on certain systems.", "title": "Supernova Cosmology with python", "description": "Certain types of exploding stars, Supernovae Type Ia have been used to infer that cosmic expansion is accelerating. Large surveys like LSST will potentially find numerous such supernovae probing the physics and phenomenology underlying the acceleration. We discuss a flexible, modular suite of python scientific software for simulation and analysis of such survey data with different algorithms. ", "time": "Saturday 1:50 p.m.-2:30 p.m."}
{"abstract": "What is deep learning? How do I start? Can I use Python? (short answer: yes!) This survey talk will include a brief overview of the past, present, and future of deep learning (what is a perceptron, what is backpropagation, and (briefly) what are some of the newer methods and their applications (drop out, convolutional nets, recurrent nets, Hinton's theory of capsules)) before addressing how you (yes, you!) can get started doing deep learning against your GPU(s) in a matter of minutes using your laptop and your favorite PyData libraries. We'll navigate the landscape of existing tools for doing deep learning with Python (Caffe, Theano, PyLearn, Graphlab-Create, etc.), and discuss some of the advantages and pitfalls of each. There will be a live demo including a few famous multi-layer neural networks (LeNet, AlexNet, and an unsupervised example such as QuocNet) trained using Python and open datasets (MNIST, ImageNet, etc.). Visualization will also be discussed. Finally, we'll touch on a few ideas that can be used as jumping-off points for real-world applications using these basic building blocks (image classification, music classification, natural language processing, automatic speech recognition, time series modeling, video game AI (via reinforcement learning), etc.).", "title": "Deep Learning with Python: getting started and getting from ideas to insights in minutes", "description": "What is deep learning? How do I start? Can I use Python? This survey talk will include a brief overview of deep learning before addressing how you can get started doing deep learning in a matter of minutes using your laptop and your favorite PyData libraries. There will be a live demo of a few famous trained DNNs, and we'll touch on a few jumping-off points for real-world applications.", "time": "Saturday 1:50 p.m.-2:30 p.m."}
{"abstract": "A good machine learning platform requires not just robust implementations of statistical models and algorithms; it also relies on having the right data structures for efficient and scalable feature engineering and data cleaning. In this talk, we discuss SFrame and SGraph, two scalable data structures designed with machine learning tasks in mind. These external memory structures make efficient use of disk and utilize a whole bag of tricks for speed. On a single machine, SFrame supports real time interactive query on terabytes of data. When used in a distributed setting, SGraph supports iterative graph analytics tasks at unparalleled speed. On a graph with 100 billions of edges, SGraph computes Pagerank at 30secs/iter with only16 EC2 machines. We walk through the architectural design and discuss tricks for scale and speed. SFrame and SGraph are the backbone of a new Python machine learning platform called GraphLab Create. Both are available for download as open source projects, or as part of the GraphLab Create binary.", "title": "SFrame and SGraph: Scalable External Memory Data Frame and Graph Structures for Machine Learning", "description": "N/A", "time": "Saturday 1:50 p.m.-2:30 p.m."}
{"abstract": "Data scientists often don't have experience working as software developers and never learned how to write unit tests for their code. They may be used to writing and executing code interactively or in an ad hoc fashion; they may be unused to writing code that runs without supervision or as part of a larger pipeline. Things get even more complicated when code has non-deterministic outcomes as is the case with probabalistic models. In this talk, I'll introduce the concept of unit testing, describe how to write good tests, and discuss how to make testing a part of a normal data science workflow. Attendees should be already familiar with the PyData stack but might not be writing production code.", "title": "Testing for Data Scientists", "description": "Data scientists often don't have experience working as software developers and never learned how to write tests for their code. Things get even more complicated when your code has non-deterministic outcomes as is the case with probabalistic models. In this talk, I'll introduce the concept of unit testing for data scientists and discuss how to make testing a part of their normal workflow.", "time": "Saturday 2:40 p.m.-3:20 p.m."}
{"abstract": "With more and more sensors readily available and collection of data becomes more ubiquitous and enables machine to machine communication(a.k.a internet of things), time series signals play more and more important role in both data collection process and also naturally in the data analysis. Data aggregation from different sources and from many people make time-series analysis crucially important in these settings. Detecting trends and patterns in time-series signals enable people to respond these changes and take actions intelligibly. Historically, trend estimation has been useful in macroeconomics, financial time series analysis, revenue management and many more fields to reveal underlying trends from the time series signals.  Trend estimation is a family of methods to be able to detect and predict tendencies and regularities in time series signals without knowing any information a priori about the signal. Trend estimation is not only useful for trends but also could yield seasonality(cycles) of data as well. Robust estimation of increasing and decreasing trends not only infer useful information from the signal but also prepares us to take actions accordingly and more intelligibly where the time of response and to action is important.  In this talk, I will introduce following trend estimation methods and compare them in real-world datasets comparing their advantages and disadvantages of each algorithm: - Moving average filtering - Exponential smoothing, - Median filtering, - Bandpass filtering, - Hodrick Prescott Filter, - Gradient Boosting Regressor, - l_1 trend filtering(my own library) ", "title": "Trend Estimation in Time Series Signals", "description": "Trend estimation is a family of methods to be able to detect and predict tendencies and regularities in time series signals without knowing any information a priori about the signal. Trend estimation is not only useful for trends but also could yield seasonality(cycles) of data as well. I will introduce various ways to detect trends in time series signals.", "time": "Saturday 2:40 p.m.-3:20 p.m."}
{"abstract": "When is Good to be Bad? How do hockey penalties affect the outcome of the game? This talk will focus more on the process of getting the penalty and goal data and less on data analysis. Although in the end, I will address the question of 'When is it good to be bad?' Looking at the source data to understand how to parse it with BeautifulSoup Beginning with the 2002-2003 season, I explored 10 seasons worth of penalty data from NHL.com. During this time, NHL.com had at least 3 formats for their play-by-play game recaps, including some that were not valid HTML formats recognized by BeautifulSoup. With BeautifulSoup, I was able to build parser that could scrape the penalty information. Using try/except to identify and accomodate of edge cases Many edge cases had to be accounted for in the parser. For example, how do I account for a shootout goal versus a regulation/overtime goal? And, how do I account for goals scored before the game starts? Using NumPy and SciPy for exploratory data analysis Once all the data was put together, I had to classify penalties. Some classifications I took from the NHL rule book (eg, Physical Fouls, Stick Infractions) and other classifications I came up with based on logic (eg, Physical Altercations, Bench, Double Minor). Penalties could belong to more than one classification. Not all seasons gave the same penalties in the pla-by-play, and rules changed between seasons. I then divided the penalties by the team score when the penalty occurred (tied, ahead, or behind), and the outcome after the penalty and calculated the odds ratio for different penalty types based on different starting points. I did a similar analysis but used 'team to score the next goal' instead of outcome in case the effect of the penalty is fleeting. All analyses are preliminary, and I need to figure out how to take into account that you need at least one member of each team for a physical altercation penalty.", "title": "When is it good to be bad? How do hockey penalties affect the outcome of the game?", "description": "On Jan. 20, Philadelphia Flyers forward Zac Rinaldo was ejected from a game after boarding Penguins defenseman Kris Letang.  The Flyers came back to win. After the game, Rinaldo said he 'changed the game' (for which he was suspended 8 games).  Using Python for webscraping and data analysis,  I explore data from 10 NHL seasons to investigate how hockey penalties affect the outcome of the game.", "time": "Saturday 2:40 p.m.-3:20 p.m."}
{"abstract": "Coming Soon", "title": "Using Python and Azure Machine Learning (Sponsor Talk)", "description": "Coming Soon", "time": "Saturday 2:40 p.m.-3:20 p.m."}
{"abstract": "Break and Snacks", "title": "Break and Snacks", "description": "Break and Snacks", "time": "Saturday 3:30 p.m.-3:45 p.m."}
{"abstract": "By using a Differences-in-Differences econometric technique we recover the causal effect of introducing a bike share program in Seattle, WA on ridership at several locations throughout the city. We control for seasonal effects as well as temperature and rainfall. We build a website to serve as an interactive walk-through of the statistical analysis. By allowing users to explore both the raw and the transformed time-series we hope to grow intuition regarding the more complicated statistics that underlay the analysis. The web framework uses off-the-shelf packages to create the website, perform the analysis and display the results. The analysis and display components are separated to allow for continuous integration in each as well as a general framework for asking and answering questions involving time-series data.", "title": "Estimating Impact on Bicycle Ridership of Bike Share Program in Seattle, WA", "description": "Using public data on bicycle ridership collected continuously at fixed locations we estimate the effect on ridership from introducing a bike share program in certain neighborhoods of Seattle, WA. The results are displayed on a hosted site and visualized using D3. The analysis is performed real-time as data updates over an API. ", "time": "Saturday 3:45 p.m.-4:25 p.m."}
{"abstract": "The Random Forest algorithm serves as a sort of Swiss Army Knife for data modeling.  It is among a handful of commonly-applied predictive techniques, and is well known both for its robustness and its predictive power.  Performance is a common drawback of the method, however, owing to a slow training phase.  The Arborist is an open-source implementation of the algorithm intended to overcome key performance bottlenecks. The Arborist's design goals include minimization of costly data movement, parallelization across a wide range of commodity hardware, language-agnostic implementation and ready internalization of common workflows.  An R-language front-end has been available for several months, and a Python version is being made available for the general machine-learning community.  Internalized workflows include quantile regression, as well as nonparametric resampling. Popular approaches to accelerating the algorithm have included distribution of training across computational nodes using such tools as MPI or Hadoop.  The Arborist augments these approaches by parallelizing training within individual trees as well across blocks, and by employing either - or both - multicore and GPGPU hardware, effecting a hierarchical parallel structure.  The ability to do this is imparted by an innovative recasting of the algorithm focused on identifying, and conserving, data locality. We outline the algorithmic structure of the Arborist and discuss some of the challenges posed in accelerating this popular technique.", "title": " Accelerating the Random Forest algorithm for commodity parallel hardware", "description": "The Arborist is an open-source implementation of the Random Forest algorithm designed for acceleration on a wide variety of hardware platforms, including multicore, multinode and GPGPU.  We examine the challenges of parallelization and present recent work extending the implementation to Python.", "time": "Saturday 3:45 p.m.-4:25 p.m."}
{"abstract": "Python 3.5, the latest installment of the language and library, is just around the corner (though you can try out the beta now). This session will cover some of the new syntax and library additions that should have people excited to start using it. As a teaser (come to the session for all the details), we'll look at better asynchronous programming, simpler mathematics, easier installation, better package management, formalized type hints, flexible function calls, and more!", "title": "What's coming in Python 3.5 (and why you should be excited)", "description": "Overview of the newest additions to Python 3.5, being released later this year.", "time": "Saturday 3:45 p.m.-4:25 p.m."}
{"abstract": "In today's online advertising marketplaces, how can websites determine which advertisers will be successful? One of the ways to monetize a website is by implementing Pay Per Click or Conversion advertising; however, this causes website owners to take on risk and potentially lose money. In this session, the AppNexus team will discuss how to solve this challenge by quickly and effectively matching the advertisers and websites using an algorithm, called the Offer Quick Test. This algorithm involves fine-tuned processes based on existing data and mathematical parameters, making it an interesting data science as well as implementation problem. Join Data Scientist, Stephanie Tzeng and Software Engineer, Sal Rinchiera as they explore these challenges to showcase how Offer Quick Test can increase ROI to the benefit of both advertisers and publishers. Learn how their in-house distributed work queue weaves data through a complex pipeline.", "title": "Brains & Brawn: the Logic and Implementation of a Redesigned Advertising Marketplace (Sponsor Talk)", "description": "In this session, the AppNexus team will discuss how their algorithm, called the Offer Quick Test, has improved the online advertising marketplace. Using fine-tuned processes based on existing data and mathematical parameters, this algorithm allows for website owners to quickly and precisely test out each advertising offer and increase the ROI for all players in the marketplace.", "time": "Saturday 3:45 p.m.-4:25 p.m."}
{"abstract": "Blaze separates expressions from computation. Odo moves complex data resources from point A to point B. Together Blaze and Odo smooth over many of the complexities of computing with large data warehouse technologies like Redshift, Impala and HDFS. Because we designed Blaze and Odo with PyData in mind they also integrate well with pandas, numpy, and a host of other foundational libraries. We show examples of both Blaze and Odo in action and discuss the design behind each library.", "title": "Blaze and Odo", "description": "Blaze and Odo are designed to help domain experts answer questions more quickly. They work together by providing a symbolic computation layer (blaze) alongside a graph of data converters (odo) that enables users to move seamlessly between formats in the most performant way. We discuss both libraries in the context of PyData and emerging data analytics technologies. ", "time": "Saturday 4:35 p.m.-5:15 p.m."}
{"abstract": "Numerical estimates of the derivative of a function are typically done using an approximation called 'Finite Difference.' In a nutshell, the derivative is a function that tells you the slope of a function at a specific point. The Finite Difference approximation takes the rise over run formula (which works exactly for linear equations), and applies it to a non-linear with a very small step size. The derivative is defined as the limit of this process, as the step size goes to zero. Computationally, however, this problem is ill-formed: floating point numbers have gaps between them, and eventually our step size becomes smaller than those gaps. This puts a hard limit on the potential accuracy of finite difference approach. Fear not! Using some fancy mathematics (the Cauchy-Riemann equations, the crown jewel of Complex Analysis) we're able to take the step in the imaginary direction on the complex plane, reducing the problem to one evaluation of the target function and a division. This results in a near perfect estimation of the derivative in only three lines of code (with a few interesting caveats)! Timeline:  (5 min) The Derivative and Finite Differences (5 min) Scipy Implementation, with benchmarks (8 min) The Cauchy Riemann Equations Overview (5 min) Use the Cauchy Riemann Equations with Finite Difference (2 min) Implementing the Complex Step Derivative in Three Lines of Code! (5 min) Examples with benchmarks (5 min) Caveats and Fixes (5 min) Q&A ", "title": "Hack the Derivative", "description": "Numerical estimates of the derivative of a function are typically done using an approximation called 'Finite Difference.' However, the accuracy of this method is computationally bounded. Using Complex Analysis, we're able to take the step in the imaginary direction on the complex plane and achieve near perfect estimation of the derivative in only three lines of code! ", "time": "Saturday 4:35 p.m.-5:15 p.m."}
{"abstract": "Everyone has an opinion on the best way to learn data science. Some people start with statistics or machine learning theory, some use R, and some use libraries like scikit-learn. I'll use several examples to contrast these with a simpler approach using functional programming techniques in Python. In addition, I'll show how even advanced data scientists can benefit from thinking more functionally.", "title": "Learning Data Science Using Functional Python", "description": "Everyone has an opinion on the best way to learn data science. Some people start with statistics or machine learning theory, some use R, and some use libraries like scikit-learn. I'll use several examples to contrast these with a simpler approach using functional programming techniques in Python. In addition, I'll show how even advanced data scientists can benefit from thinking more functionally.", "time": "Saturday 4:35 p.m.-5:15 p.m."}
{"abstract": "Coming Soon", "title": "University of Washington eScience Institute  (Sponsor Talk)", "description": "Coming Soon", "time": "Saturday 4:35 p.m.-5:15 p.m."}
{"abstract": "Implementation Strategy    Translating CPython bytecode to .NET IL    Examples of what the translation looks like    Look at what subset of IL is used    What a standardized JIT interface for CPython could look like Challenges    Places where the CLR JIT could better support Python like semantics    Places where CPython makes it difficult Performance Results    A look at various benchmarks with and without a JIT and against competing Python implementations Unsupported features    Functionality which currently isn't or can't be supported Future Directions    Future possibilities to improve performance", "title": "Building a JIT for Python", "description": "Let's make Python faster!  In a past life I spent a bunch of time working to make a fast Python implementation built on top of .NET which leveraged the CLRs JIT for improved performance.  In this talk I'll look at taking the CLR's JIT which has now been open sourced and bringing it together with the standard CPython implementation.", "time": "Saturday 5:25 p.m.-6:05 p.m."}
{"abstract": "The Community Data Science Workshops (CDSW) are a series of project-based workshops for anyone interested in learning how to use programming and data science tools to ask and answer questions about online communities like Wikipedia, Twitter, free and open source software, and civic media. The workshops are for people with no previous programming experience. The workshops bring together researchers, academics, and participants and leaders in online communities. Run three times in 2014 and 2015, the workshops have all been free of charge and are open to the public. The sessions are scheduled for one Friday evening and three Saturdays all day. Each session involves a period for lecture and technical demonstrations in the morning. The rest of the day consists of self-directed work on programming and data science projects supported by more experienced mentors. Our goal is that, after the three workshops, participants will be able to use data to produce numbers, hypothesis tests, tables, and graphical visualizations to answer questions like: Are new contributors to an article in Wikipedia sticking around longer or contributing more than people who joined last year? Who are the most active or influential users of a particular Twitter hashtag? Are people who participated in a Wikipedia outreach event staying involved? How do they compare to people that joined the project outside of the event? Our very first workshops was originally modeled after the Boston Python Workshops but most our curriculum is new and has been developed and modified by the mentors and with feedback from the participants. The CDSW curriculum, now being taught outside Seattle by others inspired by our model, is entirely based on Python. Our most recent round of workshops in Spring 2015 was taught entirely using Python 3. Teaching data science over only four days to people who begin without any familiarity with concepts like the command line or variables is a major departure from traditional data science curricula that assume at least some familiarity with programming and statistics. This talk will describe the approach we have taken to refine our material over the three times we have run the workshops and will share details of our experience. CDSW's organizers are professional programmers and data scientists and several of us have experience teaching data science in more traditional university and corporate settings. Our talk will describe how 'democratized' data science is similar to -- and sometimes extremely different from -- these more traditional approaches. We will talk about some of the challenges we have faced and highlight some of our most inspirational successes. ", "title": "Democratizing Data Science", "description": "What if programming and data science was something everybody learned? As part of the Community Data Science Workshops, more than 50 volunteers have taught than 200 complete beginners the basics of Python and data analysis in a series of 4-day workshops. We'll talk about our approach and describe the successes and challenges of approaching Python and data science as basic literacies. ", "time": "Saturday 5:25 p.m.-6:05 p.m."}
{"abstract": "Swarm intelligence (SI) algorithms mimic the collective behavior of groups such as flocks of birds and schools of fish. This session describes in detail three major SI algorithms: amoeba method optimization, particle swam optimization, and simulated bee colony optimization. Attendees will receive Python source code for each algorithm. Although SI algorithms have been studied for years, there is little practical implementation guidance available. This session describes the scenarios when SI algorithms are useful (and scenarios when SI algorithms are not useful), carefully explains how three major SI algorithms work, and presents a production quality, working demo, coded using Python, of each algorithm. Attendees will leave this session with a clear understanding of exactly what SI algorithms are, and have the knowledge needed to apply them immediately. This session assumes attendees have intermediate or higher level coding ability with Python, but does not assume any knowledge of swarm intelligence.", "title": "Swarm Intelligence Optimization using Python", "description": "See Abstract", "time": "Saturday 5:25 p.m.-6:05 p.m."}
{"abstract": "Breakfast", "title": "Breakfast", "description": "Breakfast", "time": "Sunday 8 a.m.-9 a.m."}
{"abstract": "Higher education has used analytics for a long time to guide administrative decisions. Universities are already adept at developing data-driven admissions strategies and increasingly they are using analytics in fund-raising. Learning analytics is a newer trend. Its core goal is to improve teaching, learning and student success through data. This is very appealing, but it's also fraught with complex interactions among many concerns and with disciplinary gaps between the various players.  Faculty have always collected data on students' performance on assessments and responses on surveys for the purposes of grading and complying with accreditation, sometimes also for improving teaching methods and more rarely for research on how students learn. To call it Learning Analytics, though, requires scale and some form of systemic effort.  Some early university efforts in analytics developed predictive models to identify at-risk first-year students, aiming to improve freshman retention (e.g., Purdue's 'Signals' project). Others built alert systems in support of student advising, with the goal of increasing graduation rates (e.g., Arizona State University's 'eAdvisor' system). Experts now segregate these efforts out of learning analytics, proper, because retention and graduation are not the same as learning. The goal, in that case, is to improve the function of the educational system, while learning analytics should be guided by educational research and be aimed at enhancing learning.  To elucidate what is learning analytics, it looks like we first need to answer: what is learning? What is knowledge? And can more data lead to better learning? That is perhaps the zeroth assumption of learning analytics--and it needs to be tested. There are assumptions behind any data system that go as far back as selecting what to track, where it will be tracked, how it will be collected, stored and delivered.  Most analytics is based on log data in the Learning Management System (LMS). This 'learning in a box' model is inadequate, but the diverse ecosystem of apps and services used by faculty and students poses a huge interoperability problem. The billion-dollar education industry of LMS platforms, textbook publishers and testing companies all want a part in the prospect of 'changing education' through analytics. They're all marketing their dazzling dashboards in a worrying wave of ed-tech solutionism. Meanwhile, students' every move gets tracked and logged, often without their knowledge or consent, adding ethical and legal issues of privacy for the quantified student.", "title": "Keynote: Data-driven Education and the Quantified Student", "description": "Education has seen the rise of a new trend in the last few years: Learning Analytics. This talk will weave through the complex interacting issues and concerns involving learning analytics, at a high level. The goal is to whet the appetite and motivate reflection on how data scientists can work with educators and learning scientists in this swelling field. ", "time": "Sunday 9 a.m.-9:50 a.m."}
{"abstract": "Stripe processes billions of dollars in payments a year and uses machine learning to detect and stop fraudulent transactions. Like models used for ad and search ranking, Stripe's models don't just score---they dictate actions that directly change outcomes. High-scoring transactions are blocked before they can ever get refunded or disputed by the card holder. Deploying an initial model that successfully blocks a substantial amount of fraud is a great first step, but since your model is altering outcomes, subsequent parts of the modeling process become more difficult:   How do you evaluate the model? You can't observe the eventual outcomes of the transactions you block (would they have been refunded or disputed?) or the ads you didn't show (would they have been clicked?) In general, how do you quantify the difference between the world with the model and the world without it?   How do you train new models? If your current model is blocking a lot of transactions, you have substantially fewer samples of fraud for your new training set. Furthermore, if your current model detects and blocks some types of fraud more than others, any new model you train will be biased towards detecting that residual fraud. Ideally, new models would be trained on the 'unconditional' distribution that exists in the absence of the original model.   In this talk, I'll describe how injecting a small amount of randomness in the production scoring environment allows you to answer these questions. We'll see how to obtain estimates of precision and recall (standard measures of model performance) from production data and how to approximate the distribution of samples that would exist in a world without the original model so that new models can be trained soundly.", "title": "Counterfactual evaluation of machine learning models", "description": "Machine learning models often result in actions: search results are reordered, fraudulent transactions are blocked, etc. But how do you evaluate model performance when you are altering the distribution of outcomes? I'll describe how injecting randomness in production allows you to evaluate current models correctly and generate unbiased training data for new models.", "time": "Sunday 10 a.m.-10:40 a.m."}
{"abstract": "The numpy model of computation in Python has proven to be one of the most successful ways to integrate high-performance computational code into an application. This talk offers a foundational conceptualization for this approach and discusses its strengths and limitations. ", "title": "Integration with the Vernacular", "description": "The numpy model of computation in Python has proven to be one of the most successful ways to integrate high-performance computational code into an application. This talk offers a foundational conceptualization for this approach and discusses its strengths and limitations. ", "time": "Sunday 10 a.m.-10:40 a.m."}
{"abstract": " The Big Data Analytics Pipeline viz. Collect-store-Transform-Model-Reason-Visualize-Predict-Recommend-Explore What could go wrong ? The anti-patterns Folk wisdom - Data Management & Data Science ", "title": "Big Data Analytics - The Best of the Worst : AntiPatterns & Antidotes", "description": "Let us explore some of the anti-patterns that organizations end up with in their voyage to the Big Data Nirvana. 'Data swamp', 'Technology Stampede' and 'Big data to nowhere' are all the waypoints one should avoid. Of course, we will also look at the antidotes and folk wisdom in data science as well as data management and ways to extract oneself out of the sandtraps.", "time": "Sunday 10 a.m.-10:40 a.m."}
{"abstract": "Coming Soon", "title": "State of the Library: matplotlib", "description": "Coming Soon", "time": "Sunday 10 a.m.-10:40 a.m."}
{"abstract": "The PyData ecosystem can be a bit confusing for those new to Python, or even experienced programmers moving to Python for its excellent data analysis capabilities. How do you know which tool to reach for on any given project? What tools work best for my data of size FooBar in data store FizzBuzz?  This talk will explore the Python data toolchain from bottom to top, with a focus on what tools work best based on both data locality and analysis velocity. Think of your data pipeline and storage as a city, and your data tools as a shed full of bikes. What bike works best for which trip? When should you use pure Python (the fixie) to perform your analysis? How do Pandas (the geared commuter) and Blaze (the tandem) work together? Where does Spark (the fat tire bike) fit into all of this?  This talk seeks to use questionable bike analogies to provide less-questionable look at the crowded PyData ecosystem and bring some clarity to which Python data tool is the right one to reach for on any given analysis. It will touch on pure python, toolz, Numpy, Pandas, Blaze, xray, bcolz, Dask, and Spark, with a focus on the use-cases for each one.  Finally, we'll talk about which library you should use to paint the bikeshed. ", "title": "Python Data Bikeshed", "description": "The PyData ecosystem is growing rapidly, with existing tools maturing  and exciting new tools appearing on a regular basis. This talk will examine the crowded PyData ecosystem and bring some clarity to which Python data tool is the right one to reach for on any given analysis. It will focus on use-cases for pure python, toolz, Numpy, Pandas, Blaze, xray, bcolz, Dask, and Spark.", "time": "Sunday 10:50 a.m.-11:30 a.m."}
{"abstract": "Like many Internet giants Twitter makes money by selling ads, but they've got an insidious infestation eroding their advertising credibility: bots. More than 23 million of them. Twitter bots are automatons living in the Twittersphere and ranging wildly in capability. In their simplest form, they follow you maybe fav-ing or retweeting your statuses. At their most complex, they troll and ironically, troll trolls using speech patterns that can, at times, fool humans. But when advertisers pay for engagement, they aren't interested in a four-hour flame war between a gamergate bot and a Kanye bot. When advertisers analyze social data they want to be sure their findings are the result of human activity. In Bot or Not I'll discuss the taxonomy of Twitter bots, segmenting them based on 'physical features' such as profile configuration, and on behavioral features: tweeting, retweeting and fav-ing. We'll also see how to identify bots with a classification algorithm created in scikit-learn. Finally, I'll provide some tips to advertisers on how to account for bot behavior when analyzing and interpreting results from social media experiments. I'll outline my experimental design, including the process of buying bots to create the training set. Technical details I'll discuss include using the python-twitter library to connect to the Twitter API and retrieve data, development of a bot taxonomy, and subsequent classification algorithm with pandas and scikit-learn.", "title": "Bot or Not", "description": "Twitter makes money by selling ads, but they've got an insidious infestation eroding their advertising credibility: bots. These bots are automatons living in the Twittersphere, ranging wildly in capability. In Bot or Not I'll discuss how to identify bots with a classification algorithm created in scikit-learn and provide some tips on how to account for them when analyzing social media experiments.", "time": "Sunday 10:50 a.m.-11:30 a.m."}
{"abstract": "Overview The overview will follow the general arc of an NLP project.  Reading the corpus, here this is done with gensim's streaming API. Transformations, often a transformation to BOW is done, and potentially something like TFIDF. Training the model from the corpus. Working with the result for analysis or otherwise.  Examples  This will be a straight forward application: topic discovery on a corpus and then analyzing the resulting topics to look for patterns. Next I'll cover how to use Gensim's Word2Vec implementation to better understand customer preferences. ", "title": "Low Friction NLP with Gensim", "description": "Gensim is fairly popular NLP library available in Python.  In addition to having implementations of several popular algorithms, it has a utilities that make working with the corpus itself easier. In this talk I'd like to give an overview of Gensim, and then two examples.  One will illustrate an LDA example, then I'll show a somewhat novel use of Word2Vec to understand user preferences.", "time": "Sunday 10:50 a.m.-11:30 a.m."}
{"abstract": "Coming Soon", "title": "Bokeh Dashboard Capability Use Case/Demo (Sponsor Talk)", "description": "Coming Soon", "time": "Sunday 10:50 a.m.-11:30 a.m."}
{"abstract": "A new wave of Python libraries enable interactive, scientific figures, and web shareability. From Python we can access and manipulate data with pandas, make 2D, 3D, and live-streaming graphs inside IPython Notebooks, and translate plots from static images into apps we can deploy with WebGL and D3.js.  A number of libraries support--to various degrees--plotting directly from pandas inside IPython Notebooks--matplotlib, ggplot for Python, Plotly, bokeh, cufflinks, mpld3, and Seaborn. Some of these plots can also be turned  into interactive web-based plots. This talk will show the volume of plotting options available for Python users from within these libraries, and highlight relevant technologies along the way. ", "title": "The Possibilities Of Plotting With pandas and IPython", "description": "A wave of complimentary new tools allow developers to quickly access, analyze, and plot data. IPython Notebooks let you harness these libraries and code in a web-based, language agnostic Notebook. Pandas lets you wrangle your data. And matplotlib, ggplot for Python, Plotly, bokeh, and Seaborn let you make beautiful, interactive plots. This talk shows how to use and deploy these tools together. ", "time": "Sunday 11:40 a.m.-12:20 p.m."}
{"abstract": "Space Debris are defunct objects in space, including old space vehicles (such as satellites or rocket stages) or fragments from collisions. Space debris can cause great damage to functional space ships and satellites. Thus detection of space debris and prediction of their orbital paths are essential for today's operation of space missions. The talk shows the Python based infrastructures BACARDI for gathering and storing space debris data from sensors and Skynet for high-throughput data processing and orbital collision detection.  ", "title": "High-Throughput Processing of Space Debris Data", "description": "Space Debris are defunct objects in space, including old space vehicles or fragments from collisions. Space debris can cause great damage to functional space ships and satellites. Thus detection of space debris and prediction of their orbital paths are essential. The talk shows a Python based infrastructure for storing space debris data from sensors and high-throughput processing of that data. ", "time": "Sunday 11:40 a.m.-12:20 p.m."}
{"abstract": "In determining the efficacy of energy efficiency programs at utilities, traditional methods may produce accurate assessments of energy savings but can also take considerable time to yield actionable results. With increased availability of high-quality utility program data, and using modern data analysis tools from the SciPy stack, it is now possible to predict and estimate savings much more quickly and efficiently. Machine-learning tools such as LASSO and nearest-neighbors may be used to identify the most significant drivers of program performance and assign suitable comparison groups from the full set of premises in each utility's service area, from which empirical estimates of bias and variance may be obtained. Provided with robust, granular, near-real-time information on energy savings and other performance metrics, energy efficiency programs can respond immediately to observed problems and improve program effectiveness on an ongoing basis. The result is increased savings for both the utility and its customers, as measured in both dollars and kWh.", "title": "Energy Efficiency Analysis with Python", "description": "Using Python data analysis libraries to process large volumes of utility program data, it is now possible to continuously monitor and analyze the energy savings realized from large-scale energy efficiency programs at utilities across the country, optimizing the effectiveness of these programs to help utilities and their customers save money and energy.", "time": "Sunday 11:40 a.m.-12:20 p.m."}
{"abstract": "Lunch", "title": "Lunch", "description": "Lunch", "time": "Sunday 12:20 p.m.-1:20 p.m."}
{"abstract": "Coming Soon", "title": "Keynote", "description": "Coming Soon", "time": "Sunday 1:20 p.m.-2:05 p.m."}
{"abstract": "Who's who in a developer community and what do they discuss? And with whom? This project, based on Apache Spark, provides Python pipelines for scraping, parsing, and analyzing discussion forums for a given Apache developer community -- along with analysis of related meetup events and conference talks. Messages get parsed with NLTK and TextBlob, then represented as JSON. Analytics pipelines, organized as notebooks, produce leaderboards with Spark SQL, predictive models using MLlib, and visualizations in Seaborn, while storing the data with Parquet. Code is available on GitHub.", "title": "NLP and text analytics at scale with PySpark and notebooks", "description": "Who's who in a developer community and what do they discuss? And with whom? This project, based on Apache Spark, provides Python pipelines for scraping, parsing, and analyzing discussion forums for a given Apache developer community -- along with analytics for related meetup events and conference talks.", "time": "Sunday 2:15 p.m.-2:55 p.m."}
{"abstract": "One of the most popular features of Big Data is predictive analytics. Predictive analytics is a form of business intelligence gathering. Far from the latest business buzzword, predictive analytics is a set of techniques that have become fundamental to the business strategies. In this tutorial, we will cover an example of predictive analytics through implementing a recommendation engine using python. A recommendation engine (sometimes referred to as a recommender system) is a tool that lets algorithm developers predict what a user may or may not like among a list of given items. It is very common now a day to have access to large amount of data on similar or related topics. Recommendation systems are needed to locate appropriate data on same topic or on similar topics of interest. In this talk, I will describe a recommender system framework for PubMed articles. PubMed is a free search engine that primarily accesses the MEDLINE database of references and abstracts on life-sciences and biomedical topics. The proposed recommender system produces two types of recommendations - i) content-based recommendation and (ii) recommendations based on similarities with other users' search profiles. The first type of recommendation, viz., content-based recommendation, can efficiently search for material that is similar in context or topic to the input publication. The second mechanism generates recommendations using the search history of users whose search profiles match the current user. In the talk I will present the background and motivation for these recommendation systems, and discuss the implementations of this PubMed recommendation system using python.", "title": "An example of Predictive Analytics: Building a Recommendation Engine using Python", "description": "Recommendation systems are needed to locate appropriate data on same topic or on similar topics of interest. In this talk, I will describe a recommender system framework for PubMed articles. I will present the background and motivation for these recommendation systems, and discuss the implementations of this PubMed recommendation engine with codes and examples.", "time": "Sunday 2:15 p.m.-2:55 p.m."}
{"abstract": "Statistical learning provides a set of powerful principles and tools to interpret data from many different domains. This enables insights about a range of phenomena through the construction of accurate models of the data. The application of these principles and tools to data from a specific scientific domain often presents challenges, because it requires an understanding of both the phenomena measured, as well as the properties of the measurement. In this talk, I will explore these challenges, by focusing on data from measurements of the living human brain with MRI.  Diffusion MRI (dMRI) measures water diffusion in the brain, and because tissue compartments form boundaries to free diffusion, these measurements allow us to probe the structure of the tissue, and delineate the trajectories of bundles of nerve cell projections (axons) connecting different parts of brain. Therefore, it can be used to make inferences about brain structure and connectivity, and about the tissue properties of different parts of the brain, as well as their relation to health and to cognitive abilities. Here, I focus on the use of cross-validation to compare different models of the dMRI signal. I will discuss the cross-validation API that we developed in the open-source Dipy project (http://dipy.org). This API was designed to match specific features of the data and the measurement, but also to generalize across different models. The data contains information at multiple size scales, and cross-validation can be applied at different levels, to evaluate and validate models of the microscopic distribution of fiber directions in small regions of the brain as well as long-range connections between distant brain regions.", "title": "Statistical learning of human brain structure", "description": "Statistical learning provides a set of powerful principles and tools to interpret data from many different domains, but the application of these tools to specific data often presents challenges, because it requires an understanding of both the phenomena measured, as well as the properties of the measurement.  I will explore these challenges, by focusing on measurements of the human brain with MRI.", "time": "Sunday 2:15 p.m.-2:55 p.m."}
{"abstract": "Break and Snacks", "title": "Break and Snacks", "description": "Break and Snacks", "time": "Sunday 2:55 p.m.-3:10 p.m."}
{"abstract": "(draft of detailed abstract - still in flux).  In this talk I'll cover the different ways you can index in pandas (boolean indexers, iloc, loc, ix) as well as the new query() method and how all of those elements work on MultiIndexes and time series + how to avoid issues with chained assignment and SettingWithCopy warnings.  I'll also talk about how pandas handles indexing under the hood and the implications that has for both memory usage and runtime performance.", "title": "Getting a Handle on Indexing in Pandas, Above and Below the Hood", "description": "Pandas DataFrame has a wide variety of ways to access and index data, giving lots of flexibility but also room for confusion. In this talk, we'll build up an intuition for how to index a DataFrame, explaining the pros and cons of different indexing methods (loc, iloc, ix, query, etc), how to query a MultiIndex, as well as discussing the indexing engine pandas uses behind the scenes.", "time": "Sunday 3:10 p.m.-3:50 p.m."}
{"abstract": "Bayesian inference is a powerful and flexible way to learn from data, that is easy to understand. Unfortunately larger problems are often computationally intractable. Markov Chain Monte Carlo sampling techniques help, but are still computationally limited. New gradient-based methods like the No U-Turn Sampler (NUTS) dramatically increase performance on hard problems.  PyMC 3 provides a easy and concise way to specify models and provides powerful yet easy to use samplers like NUTS. This enables users easily fit large and complex models with thousands of parameters. PyMC 3 is a complete rewrite of PyMC 2 based on Theano. PyMC expands its powerful NumPy-like syntax, and is now easier to extend and automatically optimized by Theano. We first introduce Bayesian inference and then give several examples of using PyMC 3 to show off the ease of model building and model fitting even for difficult models.", "title": "Bayesian inference with PyMC 3", "description": "PyMC 3, a total rewrite of PyMC 2, provides a powerful yet easy-to-use language for specifying statistical models and provides powerful yet easy-to-use gradient-based techniques for fitting them. New advances in sampling techniques have made it possible to fit large and complex Bayesian models much more easily than ever before and PyMC 3 is the easiest way to use them.", "time": "Sunday 3:10 p.m.-3:50 p.m."}
{"abstract": "This talk will cover some of the details of how to create a scalable point cloud system, including how floating-point Morton order can be used to spatially organize a point cloud within a globally defined octree.", "title": "Sequoia: Point Cloud Processing and Meshing", "description": "Sequoia is a product, currently in beta, from Thinkbox Software, for processing and meshing large point clouds. This talk dives into some technical details of what it takes to create a scalable point cloud system.", "time": "Sunday 3:10 p.m.-3:50 p.m."}
{"abstract": "The future is one full of intelligence. Thanks to the practical application of machine learning, apps can now drive product recommendations, predict machine failures, forecast airfare, social match-make, identify fraud, predict disease outbreaks, and repurpose pharmaceuticals. These applications output real-time predictions and recommendations in response to user and machine input to directly create intelligent services and amazing experiences which result in tremendous business value. I'll share our mission at Dato to accelerate the development of intelligent applications. There will be code and demos.  ", "title": "Creating an intelligent world at Dato.(Sponsor Talk)", "description": "The future is one full of intelligence. Thanks to the practical application of machine learning, apps can now drive product recommendations, predict machine failures, forecast airfare, social match-make, identify fraud, predict disease outbreaks, and repurpose pharmaceuticals. I'll share our mission at Dato to accelerate the development of intelligent applications. There will be code and demos.  ", "time": "Sunday 3:10 p.m.-3:50 p.m."}
{"abstract": "In this humbling talk, I'll describe some mistakes I've made in working in statistics and machine learning. I'll describe my original intentions, symptoms, how I eventually discovered the mistake, and possibly even a solution. The topics include mistakes in A/B testing, Kaggle competitions, data collection, and other fields. I'll also introduce some interesting statistical and machine learning counterexamples: examples where our original intuition fails, and solutions to these examples. ", "title": "Mistakes I've Made", "description": "In this humbling talk, I'll describe some mistakes I've made in working in statistics and machine learning. I'll describe my original intentions, symptoms, how I eventually discovered the mistake, and possibly even a solution. The topics include mistakes in A/B testing, Kaggle competitions, data collection, and other fields.", "time": "Sunday 4 p.m.-4:40 p.m."}
{"abstract": "Companies can't stop gushing about how 'data-driven' they are - how they're using 'big data' and 'data science' to synergize and streamline all the things. But being driven by data alone is a flawed approach. Instead, companies should seek to be 'data-informed' - interweaving designers, UXers, and data scientists so that each side is able to perfectly complement the one another. This talk will discuss the importance of allowing data and user research to complement one another, in addition to the pitfalls of being driven by data alone (for instance, the cons of A/B testing).", "title": "Why 'data-informed' beats 'data-driven.", "description": "Companies can't stop gushing about how 'data-driven' they are - how they're using 'big data' and 'data science' to synergize and streamline all the things. But being driven by data alone is a flawed approach; instead, companies should seek to be 'data-informed' by interweaving designers, UXers, and data scientists so that each side is able to perfectly complement the others.", "time": "Sunday 4 p.m.-4:40 p.m."}
{"abstract": "This project focus on how social media data extracted about the fast food industry can simply and repeatably be turned into a system for analyzing market position.  This market position can by proxy be used to derive strategy.   Data will be extracted from Twitter for a variety of brands that operate in the same market space. That data will be split into its individual words and using nltk it will be cast to its root word. A TFIDF (sci-kit or raw code) analysis will be used to identify which words are uniquely used to describe the industry. The data will then be topic modeled in gensim to further reduce the number of dimensions and to reduce the number of feature vectors that need to be tracked. Finally the data will be reduced into a 4-plot using Correspondence Analysis or MDS.    While these are all independently simple the analysis itself is extensible enough to solve this problem in a variety of circumstances.           ", "title": "Social Media Brand Positioning Workflow", "description": "Social Media is becoming ever pervasive in  modern culture.    One of the simplest use cases of social data is as a 'temperature check' for how a brand is performing.  This talk offers a simple walk-through of how python can be used  to take Social Data from its raw form and transform it into a usable visualization to help understand the market a company exists in.  ", "time": "Sunday 4 p.m.-4:40 p.m."}
{"abstract": "Lightning Talks ", "title": "Lightning Talks", "description": "Lightning talks", "time": "Sunday 4:50 p.m.-5:30 p.m."}