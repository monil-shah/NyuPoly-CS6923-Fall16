{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering text documents using k-means\n",
    "Example from [sklearn documentation](http://scikit-learn.org/stable/auto_examples/text/document_clustering.html) by Peter Prettenhofer and Lars Buitinck.\n",
    "\n",
    "This is an example showing how the scikit-learn can be used to cluster\n",
    "documents by topics using a bag-of-words approach. This example uses\n",
    "a scipy.sparse matrix to store the features instead of standard numpy arrays.\n",
    "\n",
    "It can be noted that k-means is very sensitive to\n",
    "feature scaling and that in this case the IDF weighting helps improve the\n",
    "quality of the clustering by quite a lot as measured against the \"ground truth\"\n",
    "provided by the class label assignments of the 20 newsgroups dataset.\n",
    "\n",
    "Note: as k-means is optimizing a non-convex objective function, it will likely\n",
    "end up in a local optimum. Several runs with independent random init might be\n",
    "necessary to get a good convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20-newsgroups dataset for categories: ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387 article in 4 categories\n",
      "CPU times: user 649 ms, sys: 67.7 ms, total: 716 ms\n",
      "Wall time: 725 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space',]\n",
    "print \"Loading 20-newsgroups dataset for categories:\", categories\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True)\n",
    "print \"%d article in %d categories\" % (len(dataset.data), len(dataset.target_names))\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction\n",
    "Extract features from the training dataset using a sparse vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions feature matrix: (3387, 10000)\n",
      "CPU times: user 928 ms, sys: 42 ms, total: 970 ms\n",
      "Wall time: 950 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, min_df=2, max_df=0.5)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print \"Dimensions feature matrix:\", X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction\n",
    "We could reduce the number of dimensions by performing a SVD (see next lesson). We would loose the actual meaning of the words in each cluster. We skip this step for now, since we haven't covered SVD yet, and we would like to inspect the words in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set None for skipping dimension reduction, otherwise choose an integer\n",
    "n_components = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if n_components is not None:\n",
    "    print \"Performing dimensionality reduction using LSA\"\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(n_components)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "    X_red = lsa.fit_transform(X)\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print \"Explained variance of the SVD step: %d%%\" % (explained_variance * 100)\n",
    "    print \"Dimensions feature matrix:\", X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if n_components is not None:\n",
    "    X = X_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying k-means\n",
    "Do the actual clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 2.6 ms, total: 254 ms\n",
      "Wall time: 254 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.572\n",
      "Completeness: 0.645\n",
      "V-measure: 0.606\n",
      "Adjusted Rand-Index: 0.566\n",
      "Silhouette Coefficient: 0.007\n"
     ]
    }
   ],
   "source": [
    "print \"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_)\n",
    "print \"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_)\n",
    "print \"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_)\n",
    "print \"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_)\n",
    "print \"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, km.labels_, sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show some words in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 terms per cluster:\n",
      "Cluster 0: sgi keith livesey wpd solntze jon caltech com schneider morality allan cco\n",
      "Cluster 1: graphics university com image thanks posting host nntp ac computer file 3d\n",
      "Cluster 2: space nasa henry access digex toronto gov pat alaska shuttle com moon\n",
      "Cluster 3: god com sandvik people jesus article don say christian bible religion believe\n"
     ]
    }
   ],
   "source": [
    "top = 12\n",
    "if not n_components:\n",
    "    print \"Top %d terms per cluster:\" % top\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print \"Cluster %d:\" % i,\n",
    "        for ind in order_centroids[i, :top]:\n",
    "            print terms[ind],\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
